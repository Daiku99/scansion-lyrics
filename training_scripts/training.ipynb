{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830df39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, AdamW, MBartConfig\n",
    "\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b978fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, MBartConfig)\n",
    "\n",
    "# from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "# from seq2seq_training_args import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe3a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\",\n",
    "                                           add_prefix_space=True, \n",
    "                                           src_lang=\"zh_CN\",\n",
    "                                           tgt_lang=\"zh_CN\")\n",
    "\n",
    "#special tokens\n",
    "content_tokens = [\"<c\"+str(x)+\">\" for x in range(0,300)]\n",
    "rhyme_tokens = [\"<b\"+str(x)+\">\" for x in range(0,4)]\n",
    "new_tokens = [\"<h>\",\"<l>\",\"<d>\",\"<m>\"]\n",
    "new_tokens.append(\"<|sep|>\")\n",
    "new_tokens.append(\"<|num|>\")\n",
    "new_tokens.append(\"<|bdr|>\")\n",
    "new_tokens.append(\"<|kwd|>\")\n",
    "new_tokens += content_tokens\n",
    "new_tokens += rhyme_tokens\n",
    "\n",
    "tokenizer.add_tokens(list(new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8074b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def contains_japanese_or_korean(text):\n",
    "    # Regular expression pattern for Japanese and Korean characters\n",
    "    pattern = re.compile(r'[\\p{Script=Hiragana}\\p{Script=Katakana}\\p{Script=Hangul}]+')\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = pattern.search(text)\n",
    "    \n",
    "    # If a match is found, return True; otherwise, return False\n",
    "    return match is not None\n",
    "\n",
    "# Example usage:\n",
    "text1 = \"こんにちは 안녕하세요\"\n",
    "text2 = \"Hello, World!\"\n",
    "text3 = \"今天\"\n",
    "\n",
    "print(contains_japanese_or_korean(text1))  # True\n",
    "print(contains_japanese_or_korean(text3))  # False\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    # initializing punctuations string\n",
    "    punc = '''!()-[]{};:'\"\\,./?@#$%^&*_~。，、！（）【】？'''\n",
    "    # Removing punctuations in string\n",
    "    # Using loop + punctuation string\n",
    "    for ele in string:\n",
    "        if ele in punc:\n",
    "            string = string.replace(ele, \"\")\n",
    "    return string\n",
    "\n",
    "#training_data\n",
    "data = list()\n",
    "\n",
    "with open(\"./L_scasion_training_data/three_height_scansion_wb_tones.csv\",\"r\") as fin:\n",
    "    lines = [line.strip() for line in fin.readlines()]\n",
    "    for sent in lines[1:]:\n",
    "        sent = sent.split(\"\\t\")\n",
    "        keyword,wb, tones, lyric = sent[0],sent[1],sent[2],sent[3]\n",
    "        if len(keyword) == 0:\n",
    "            print(\"0 keywords\")\n",
    "        else:\n",
    "            lyr = remove_punctuation(lyric)\n",
    "            if 1 < len(lyr) < 20:\n",
    "                if contains_japanese_or_korean(lyr):\n",
    "                    pass\n",
    "                else:\n",
    "                    num =  [\"<c\" + str(x) + \">\" for x in range(len(lyr))]\n",
    "                    d = keyword + \" <|kwd|> \" + \" \".join(num)  + \" <|num|> \"+ wb + \" <bdr> \" + tones + \" <|sep|> \" + lyr \n",
    "                    data.append(d)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70279eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877868 877868\n"
     ]
    }
   ],
   "source": [
    "dataset = data\n",
    "print(len(dataset),len(set(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b34603-429a-49c4-833e-23d2494d6b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'款人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <c11> <c12> <c13> <c14> <c15> <|num|> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b0> <b1> <b1> <b1> <bdr> <l> <l> <h> <l> <m> <h> <h> <m> <l> <h> <l> <m> <m> <l> <l> <h> <|sep|> 我讲这款人这世人的爱比人什么卡多'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be75e578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250339, 1024)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250339, 1024)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250339, 1024)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250339, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\", \n",
    "                                                            cache_dir=\"/home/yc429/rds/hpc-work/jupyter/cache\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "mbart_model.resize_token_embeddings(len(tokenizer))\n",
    "mbart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c7aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_dict = dict()\n",
    "for d in dataset:\n",
    "    line = d.split('<|sep|>')\n",
    "    try:\n",
    "        source, target = line[0], line[1].strip() #changed line[1:]\n",
    "        CN_dict[source] = target\n",
    "    except IndexError:\n",
    "        print(line)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d36b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size of data is 814242\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for src, tgt in CN_dict.items():\n",
    "    data.append(\n",
    "          {\n",
    "              \"translation\": {\n",
    "                  \"hi\": str(src),\n",
    "#                   \"en\": ' '.join(tgt)\n",
    "                  \"en\":str(tgt)\n",
    "              }\n",
    "          }\n",
    "      )\n",
    "\n",
    "print(f'total size of data is {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd34716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'hi': '款人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <c11> <c12> <c13> <c14> <c15> <|num|> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b0> <b1> <b1> <b1> <bdr> <l> <l> <h> <l> <m> <h> <h> <m> <l> <h> <l> <m> <m> <l> <l> <h> ',\n",
       "   'en': '我讲这款人这世人的爱比人什么卡多'}},\n",
       " {'translation': {'hi': '格调 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <c11> <c12> <c13> <|num|> <b0> <b0> <b0> <b0> <b0> <b1> <b1> <b0> <b1> <b1> <b0> <b1> <b0> <b1> <bdr> <l> <h> <h> <l> <m> <l> <l> <m> <l> <h> <m> <h> <h> <h> ',\n",
       "   'en': '百分之九十九点九九都格调太低'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b46b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source and translated texts from the raw data\n",
    "source_texts = [entry['translation']['hi'] for entry in data]\n",
    "translated_texts = [entry['translation']['en'] for entry in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543b35d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814242 814242\n"
     ]
    }
   ],
   "source": [
    "print(len(source_texts),len(translated_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6426673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['款人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <c11> <c12> <c13> <c14> <c15> <|num|> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b1> <b1> <b0> <b1> <b0> <b1> <b1> <b1> <bdr> <l> <l> <h> <l> <m> <h> <h> <m> <l> <h> <l> <m> <m> <l> <l> <h> ', '格调 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <c11> <c12> <c13> <|num|> <b0> <b0> <b0> <b0> <b0> <b1> <b1> <b0> <b1> <b1> <b0> <b1> <b0> <b1> <bdr> <l> <h> <h> <l> <m> <l> <l> <m> <l> <h> <m> <h> <h> <h> ', '伤口 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b1> <b0> <b1> <b1> <b0> <b1> <bdr> <m> <h> <h> <m> <l> <h> <l> ', '结果 <|kwd|> <c0> <c1> <c2> <c3> <c4> <|num|> <b0> <b1> <b1> <b0> <b1> <bdr> <m> <l> <h> <m> <h> ', '桃花羞 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <bdr> <h> <h> <m> <h> <h> <h> <h> '] ['我讲这款人这世人的爱比人什么卡多', '百分之九十九点九九都格调太低', '不会愈合的伤口', '结果是遗憾', '只见桃花羞答答']\n"
     ]
    }
   ],
   "source": [
    "print(source_texts[:5],translated_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86444791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train, validation\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "split = 0.1\n",
    "train_dataset, eval_dataset = random_split(data, lengths=[int((1-split)*len(data)), len(data)-int((1-split)*len(data))])\n",
    "\n",
    "# defining collator functioon for preparing batches on the fly .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952534e6-6209-44a9-b9fe-410cd620b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features:list):\n",
    "\n",
    "  labels = [f[\"translation\"][\"en\"] for f in features]\n",
    "  inputs = [f[\"translation\"][\"hi\"] for f in features]\n",
    "\n",
    "  batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, src_lang=\"zh_CN\", tgt_lang=\"zh_CN\", tgt_texts=labels, max_length=128, max_target_length=128)\n",
    "\n",
    "  for k in batch:\n",
    "    batch[k] = torch.tensor(batch[k])\n",
    "\n",
    "  return batch\n",
    "\n",
    "# data_collator(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300524a0-496d-4958-a44f-e28dee4d548c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./mbart_3_height_wb_no_poem_epoch3/checkpoint-68000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250339, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250339, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250339, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250339, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's assume this is your output directory.\n",
    "output_dir = \"./mbart_3_height_wb_no_poem_epoch3\"\n",
    "\n",
    "# Find all subdirectories of output_dir.\n",
    "subdirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "# Sort them by last modification time.\n",
    "subdirs = sorted(subdirs, key=os.path.getmtime)\n",
    "\n",
    "# The last checkpoint is the most recent one.\n",
    "last_checkpoint = subdirs[-1]\n",
    "\n",
    "print(last_checkpoint)\n",
    "\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mbart_model.resize_token_embeddings(len(tokenizer))\n",
    "mbart_model.to(device)\n",
    "mbart_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c7c3d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./mbart_3_height_wb_no_poem_epoch3/checkpoint-66000).\n",
      "***** Running training *****\n",
      "  Num examples = 732817\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68703\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 2\n",
      "  Continuing training from global step 66000\n",
      "  Will skip the first 2 epochs then the first 20198 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
      "Skipping the first batches:   0%|                             | 0/20198 [00:00<?, ?it/s]/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3415: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "Skipping the first batches: 100%|████████████████| 20198/20198 [03:16<00:00, 102.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68703' max='68703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68703/68703 45:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>0.709867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>0.709360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.708955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.743400</td>\n",
       "      <td>0.708723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.759200</td>\n",
       "      <td>0.708515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 81425\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81425\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_wb_no_poem_epoch3/checkpoint-67000\n",
      "Configuration saved in ./mbart_3_height_wb_no_poem_epoch3/checkpoint-67000/config.json\n",
      "Model weights saved in ./mbart_3_height_wb_no_poem_epoch3/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_wb_no_poem_epoch3/checkpoint-62000] due to args.save_total_limit\n",
      "/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3415: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81425\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81425\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_wb_no_poem_epoch3/checkpoint-68000\n",
      "Configuration saved in ./mbart_3_height_wb_no_poem_epoch3/checkpoint-68000/config.json\n",
      "Model weights saved in ./mbart_3_height_wb_no_poem_epoch3/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_wb_no_poem_epoch3/checkpoint-63000] due to args.save_total_limit\n",
      "/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3415: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 81425\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=68703, training_loss=0.029603517959692672, metrics={'train_runtime': 2916.7184, 'train_samples_per_second': 753.741, 'train_steps_per_second': 23.555, 'total_flos': 2.726523842875392e+17, 'train_loss': 0.029603517959692672, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_3_height_wb_no_poem_epoch3\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# Create the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # You can define a data collator if needed\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    "    \n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "308f9027-2af3-4dd2-9580-f4a688f62aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./L_scansion_models/mbart_3_height_wb_no_poem_epoch3/config.json\n",
      "Model weights saved in ./L_scansion_models/mbart_3_height_wb_no_poem_epoch3/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "mbart_model.save_pretrained('./L_scansion_models/mbart_3_height_wb_no_poem_epoch3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e560b24f-1721-4a1b-8640-7419051436a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250339, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250339, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250339, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250339, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./L_scansion_models/mbart_3_height_wb_no_poem_epoch3\"\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mbart_model.resize_token_embeddings(len(tokenizer))\n",
    "mbart_model.to(device)\n",
    "\n",
    "mbart_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2afabf6-6339-4b3c-a321-5c855f46e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list()\n",
    "with open(\"/home/yc429/Ch_lyrics_generation/12_new_melodies/12_prompts.txt\",\"r\") as fin:\n",
    "    for line in fin.readlines()[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        melody_idx, keyword, length, wb, contour = line[0],line[1],int(line[2]),line[3],line[4]\n",
    "        prompt = keyword + \" <|kwd|> \" + \" \".join([\"<c\"+str(x)+\">\" for x in range(length)]) + \" <|num|> \" + wb + \" <bdr> \" + contour\n",
    "        prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16a3314-bc72-4272-9a64-14da009aeb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<b0> <b0> <b0> <b1> <b0> <b0> <b1>', '<b0> <b0> <b0> <b1> <b0> <b0> <b1>', '<b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b1> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0>', '<b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1>', '<b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1>', '<b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1>', '<b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b1> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b1> <b0> <b0> <b0> <b0>', '<b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b1>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b1>', '<b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1>', '<b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0>', '<b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0>', '<b0> <b0> <b0> <b1> <b0> <b0> <b0>']\n"
     ]
    }
   ],
   "source": [
    "def read_file_to_list(filepath):\n",
    "    \"\"\"\n",
    "    Read a file and return each line as an element in a list.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list where each element is a line from the file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        # Read lines and strip newline characters\n",
    "        lines = [line.strip().split(\"\\t\")[1] for line in file]\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Example usage:\n",
    "filepath = '/home/yc429/Ch_lyrics_generation/10_old_melodies/10_old_duration_wb_rule.txt'\n",
    "lines_list = read_file_to_list(filepath)\n",
    "print(lines_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fced4e61-0c3f-41f6-9b8c-88e78444df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list()\n",
    "with open(\"/home/yc429/Ch_lyrics_generation/10_old_melodies/3_height_similarity_10_melody_prompts.txt\",\"r\") as fin:\n",
    "    for line in fin.readlines()[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        prompt = line[1]\n",
    "        prompts.append(prompt)\n",
    "        # melody_idx, keyword, length, wb, contour = line[0],line[1],int(line[2]),line[3],line[4]\n",
    "        # prompt = keyword + \" <|kwd|> \" + \" \".join([\"<c\"+str(x)+\">\" for x in range(length)]) + \" <|num|> \" + wb + \" <bdr> \" + contour\n",
    "        # prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5ddd54-0509-4eca-9516-572dc2e15fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_wb = list()\n",
    "for i in range(len(prompts)):\n",
    "    p = prompts[i].split(\"<|num|>\")\n",
    "    tmp = p[0] + \"<|num|> \" + lines_list[i] + \" <bdr>\" + p[1]\n",
    "    prompts_wb.append(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a75d058-23de-4188-9d02-be77b0ba7877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3415: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     6,   3759,   5148, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250335, 250335, 250335, 250336, 250335, 250335,\n",
      "         250336,   4426,  34145,     42,   2740, 250030, 250028, 250030, 250027,\n",
      "         250027, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   3759,   5148, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250335, 250335, 250335, 250336, 250335, 250335,\n",
      "         250336,   4426,  34145,     42,   2740, 250030, 250028, 250030, 250027,\n",
      "         250027, 250027, 250027, 250031,      2, 250025]])\n",
      "{'无情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <h> <h> <|sep|>': ['无语无泪爱上毒', '无语无声爱上毒', '无语无声爱上你', '无语无泪爱不义', '无语无泪心花瓣']}\n",
      "{'input_ids': tensor([[     6, 134479,   1553, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250335, 250335, 250336, 250335,\n",
      "         250335, 250336,   4426,  34145,     42,   2740, 250030, 250028, 250030,\n",
      "         250027, 250027, 250030, 250027, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 134479,   1553, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250335, 250335, 250336, 250335,\n",
      "         250335, 250336,   4426,  34145,     42,   2740, 250030, 250028, 250030,\n",
      "         250027, 250027, 250030, 250027, 250028, 250031,      2, 250025]])\n",
      "{'泪水 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <m> <h> <l> <|sep|>': ['流血流汗流泪水滴', '难舍难分流泪水淌', '无悔无怨流泪水滴', '无法平息流泪水', '好久不见流泪水']}\n",
      "{'input_ids': tensor([[     6,   5003,   4183, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250335, 250335, 250335, 250336, 250335, 250335, 250336,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250027, 250027, 250030,\n",
      "         250028, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   5003,   4183, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250335, 250335, 250335, 250336, 250335, 250335, 250336,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250027, 250027, 250030,\n",
      "         250028, 250030, 250031,      2, 250025]])\n",
      "{'眼神 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <l> <m> <|sep|>': ['含苞欲放迷了眼神', '不屑一顾谁的眼神', '含苞欲放红眼神', '不屑一顾无眼神', '含苞欲放红眼神迷']}\n",
      "{'input_ids': tensor([[     6,   7149,   9731, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250030, 250030,\n",
      "         250030, 250030, 250028, 250028, 250028, 250030, 250028, 250031,      2,\n",
      "         250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   7149,   9731, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250030, 250030,\n",
      "         250030, 250030, 250028, 250028, 250028, 250030, 250028, 250031,      2,\n",
      "         250025]])\n",
      "{'风雨 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b0> <b0> <bdr> <h> <m> <m> <m> <m> <l> <l> <l> <m> <l> <|sep|>': ['风雨来来往往我无所畏', '任凭寒来暑往我们迎着风', '任凭来来回回的有雨有风', '任凭来来回回的有无有', '任凭来来回回的有雨淋着风']}\n",
      "{'input_ids': tensor([[     6,  19015, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250030, 250030, 250030, 250028, 250028, 250027,\n",
      "         250028, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  19015, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250030, 250030, 250030, 250028, 250028, 250027,\n",
      "         250028, 250028, 250031,      2, 250025]])\n",
      "{'结果 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <h> <m> <m> <m> <m> <l> <l> <h> <l> <l> <|sep|>': ['到头来来结果三百九十九', '到头来来结果一场赌注', '到头来来结果三百五十分', '到头来来结果三百五千', '到头来来结果三九九九']}\n",
      "{'input_ids': tensor([[     6,  64566, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250336,\n",
      "         250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250027, 250027, 250030, 250030, 250028, 250030, 250027, 250030,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  64566, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250336,\n",
      "         250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250027, 250027, 250030, 250030, 250028, 250030, 250027, 250030,\n",
      "         250031,      2, 250025]])\n",
      "{'心情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <m> <l> <m> <h> <m> <|sep|>': ['离家出没来久而心情烦', '年复一年烦杂人心情烦', '年复一年烦杂人心情', '离家出没来久别心情烦', '离家出没来起伏心情起伏']}\n",
      "{'input_ids': tensor([[     6,   9770, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250336,\n",
      "         250335, 250335, 250336, 250335,   4426,  34145,     42,   2740, 250030,\n",
      "         250028, 250030, 250027, 250027, 250027, 250027, 250027, 250027, 250031,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   9770, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250336,\n",
      "         250335, 250335, 250336, 250335,   4426,  34145,     42,   2740, 250030,\n",
      "         250028, 250030, 250027, 250027, 250027, 250027, 250027, 250027, 250031,\n",
      "              2, 250025]])\n",
      "{'朋友 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <h> <h> <|sep|>': ['朋友难聚聚聚聚聚散散', '朋友来作伴肩并肩作战', '朋友来作伴肩并肩站稳', '朋友和睦睦笑一笑开怀', '朋友难聚聚聚聚散散聚']}\n",
      "{'input_ids': tensor([[     6,   9524,  14996, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   9524,  14996, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250028, 250031,      2, 250025]])\n",
      "{'思念 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <h> <m> <l> <m> <m> <m> <m> <m> <l> <l> <|sep|>': ['蓦然回眸来来回回的思念', '蓦然回眸茫茫人海里头', '蓦然回眸朦朦胧胧想念', '蓦然回眸朦朦胧胧思念', '蓦然回眸茫茫人海里思']}\n",
      "{'input_ids': tensor([[     6,  91453, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250336,\n",
      "         250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250027, 250030, 250027,\n",
      "         250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  91453, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250336,\n",
      "         250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250027, 250030, 250027,\n",
      "         250027, 250027, 250031,      2, 250025]])\n",
      "{'日子 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <m> <h> <m> <h> <h> <h> <|sep|>': ['祝福祖国年少无知日子', '多年以前同甘共命运日子', '祝福祖国和衷共济日子', '祝福祖国繁荣富强日子', '祝福祖国平淡无味日子']}\n",
      "{'input_ids': tensor([[     6,   3221, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250336,\n",
      "         250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   3221, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250336,\n",
      "         250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,\n",
      "           2740, 250027, 250030, 250028, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250028, 250031,      2, 250025]])\n",
      "{'世界 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <m> <m> <m> <m> <l> <l> <|sep|>': ['既然选择环游世界的五彩', '既然选择环游世界的顶端', '终于醒来原来如此而已', '祝福祖国繁荣民主民主', '未来属于贫穷穷国家的']}\n",
      "{'input_ids': tensor([[     6, 141390, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250336, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335, 250335, 250336,   4426,  34145,     42,   2740, 250030,\n",
      "         250030, 250027, 250028, 250030, 250030, 250028, 250028, 250031,      2,\n",
      "         250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 141390, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250336, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335, 250335, 250336,   4426,  34145,     42,   2740, 250030,\n",
      "         250030, 250027, 250028, 250030, 250030, 250028, 250028, 250031,      2,\n",
      "         250025]])\n",
      "{'时光 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <m> <h> <l> <m> <m> <l> <l> <|sep|>': ['随时光转红红火火', '随时光转来来往往', '原来真的时光荏苒苒苒', '随时光转十百转百转', '随时光转来来来往往']}\n",
      "{'input_ids': tensor([[     6,  25099, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250336, 250335, 250335, 250335,\n",
      "         250336, 250335,   4426,  34145,     42,   2740, 250030, 250028, 250030,\n",
      "         250027, 250027, 250027, 250027, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  25099, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250336, 250335, 250335, 250335,\n",
      "         250336, 250335,   4426,  34145,     42,   2740, 250030, 250028, 250030,\n",
      "         250027, 250027, 250027, 250027, 250030, 250031,      2, 250025]])\n",
      "{'女人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <m> <|sep|>': ['男女人真心真意疼惜', '男女人一去不复回首', '男女人真心真意疼', '男女人自甘堕落无情', '男女人一去不复回']}\n",
      "{'input_ids': tensor([[     6, 129954, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250027, 250027, 250030, 250027, 250030,\n",
      "         250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 129954, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250027, 250027, 250030, 250027, 250030,\n",
      "         250028, 250031,      2, 250025]])\n",
      "{'笑容 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <h> <m> <l> <|sep|>': ['面带笑容泪如雨下', '面带笑容义无反顾', '面带笑容泪流满面', '面带笑容泪含着雨', '面带笑容涕红了眼']}\n",
      "{'input_ids': tensor([[     6,  71979, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250030, 250030, 250030, 250027, 250027,\n",
      "         250027, 250027, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  71979, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250030, 250030, 250030, 250027, 250027,\n",
      "         250027, 250027, 250030, 250031,      2, 250025]])\n",
      "{'爱情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <m> <h> <h> <h> <h> <m> <|sep|>': ['谈何容易见真爱情', '谈何容易失去爱情', '谈谈情说爱爱情', '谈何容易见怪爱情', '谈谈情说说爱情']}\n",
      "{'input_ids': tensor([[     6, 102521, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250030, 250028, 250030, 250027, 250027,\n",
      "         250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 102521, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "           4426,  34145,     42,   2740, 250030, 250028, 250030, 250027, 250027,\n",
      "         250027, 250031,      2, 250025]])\n",
      "{'声音 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <l> <m> <h> <h> <h> <|sep|>': ['好久不见声音沙哑', '潮起潮落声音嘶哑', '来来回去声音嘶哑', '十指连心花怒放声音', '来来回回声音嘶哑']}\n",
      "{'input_ids': tensor([[     6,  26379, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250027, 250027,\n",
      "         250028, 250028, 250030, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  26379, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250027, 250027,\n",
      "         250028, 250028, 250030, 250027, 250027, 250031,      2, 250025]])\n",
      "{'感觉 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <l> <l> <m> <h> <h> <|sep|>': ['冥冥之中有种奇妙', '甜甜蜜蜜的感觉', '情不自禁有种奇妙', '寻寻觅觅总有极限期', '寻寻觅觅总找不到家']}\n",
      "{'input_ids': tensor([[     6,   4695, 134479, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250335, 250335,   4426,  34145,     42,\n",
      "           2740, 250030, 250030, 250027, 250027, 250028, 250027, 250027, 250027,\n",
      "         250027, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   4695, 134479, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250335, 250335,   4426,  34145,     42,\n",
      "           2740, 250030, 250030, 250027, 250027, 250028, 250027, 250027, 250027,\n",
      "         250027, 250030, 250031,      2, 250025]])\n",
      "{'流泪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <l> <h> <h> <h> <h> <m> <|sep|>': ['流流泪泪喜怒哀乐无常', '流年似箭矢志不渝流泪', '流流泪泪洗面具具足矣', '流年似箭矢志不屈为流泪', '流流泪泪洗面具具具足矣']}\n",
      "{'input_ids': tensor([[     6,  60871, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250336, 250335, 250336, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250030, 250027, 250027, 250028, 250028, 250030, 250027, 250027,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  60871, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250336, 250335, 250336, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250030, 250027, 250027, 250028, 250028, 250030, 250027, 250027,\n",
      "         250031,      2, 250025]])\n",
      "{'时候 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b1> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <m> <h> <h> <l> <l> <m> <h> <h> <|sep|>': ['明明知道我们没说出时候', '明明知道我们来时时候', '明明知道我们没说个时候', '明明知道我们没关系时候', '临别时候我们绝不会分手']}\n",
      "{'input_ids': tensor([[     6,  38717, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250027, 250027,\n",
      "         250030, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  38717, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250027, 250027,\n",
      "         250030, 250027, 250031,      2, 250025]])\n",
      "{'眼睛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <m> <h> <|sep|>': ['迷迷糊糊睁不开眼睛', '迷迷糊糊蒙蒙眼睛', '朦朦胧胧睁不开眼睛', '迷迷糊糊糊蒙上眼睛', '红着眼睛不吭不吭']}\n",
      "{'input_ids': tensor([[     6,  65503, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250027, 250030, 250028, 250030,\n",
      "         250027, 250027, 250027, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  65503, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250027, 250030, 250028, 250030,\n",
      "         250027, 250027, 250027, 250027, 250027, 250031,      2, 250025]])\n",
      "{'梦想 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <m> <l> <m> <h> <h> <h> <h> <h> <|sep|>': ['追寻所求之光为梦想', '欢迎你来加入为梦想', '追寻所求之光放梦想', '追寻所求之光放飞梦想', '追寻理想乘风破浪为梦想']}\n",
      "{'input_ids': tensor([[     6,   6717,  11158, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250027, 250030, 250028, 250030, 250027, 250027, 250027, 250027, 250027,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   6717,  11158, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250027, 250030, 250028, 250030, 250027, 250027, 250027, 250027, 250027,\n",
      "         250031,      2, 250025]])\n",
      "{'世间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <h> <m> <l> <m> <h> <h> <h> <h> <h> <|sep|>': ['再回首来生看世间变迁', '再回首来生看世间沧桑', '再回首来时看世间变迁', '酸甜苦辣看世间变迁', '再回首来生看世间苍老']}\n",
      "{'input_ids': tensor([[     6,  10491, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250335, 250335, 250336, 250335,\n",
      "         250335, 250336, 250335,   4426,  34145,     42,   2740, 250030, 250027,\n",
      "         250027, 250030, 250030, 250030, 250027, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  10491, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250335, 250335, 250335, 250336, 250335,\n",
      "         250335, 250336, 250335,   4426,  34145,     42,   2740, 250030, 250027,\n",
      "         250027, 250030, 250030, 250030, 250027, 250028, 250031,      2, 250025]])\n",
      "{'地方 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <m> <m> <h> <l> <|sep|>': ['无论在何时同一个地方', '无论到何时同一个地方', '不论到何时停不了地方', '无论到何时停不了地方', '不论到何时同一个地方']}\n",
      "{'input_ids': tensor([[     6, 211284, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250030, 250030,\n",
      "         250027, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 211284, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250030, 250030, 250030,\n",
      "         250027, 250027, 250027, 250031,      2, 250025]])\n",
      "{'流浪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <m> <m> <h> <h> <h> <|sep|>': ['茫茫人海中漂荡', '何时何地流浪街头', '茫茫人海中爱恨情', '茫茫人海中爱上流浪', '茫茫人海中之道者']}\n",
      "{'input_ids': tensor([[     6,  95490, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250027,\n",
      "         250027, 250030, 250030, 250030, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  95490, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250027,\n",
      "         250027, 250030, 250030, 250030, 250027, 250027, 250031,      2, 250025]])\n",
      "{'天空 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <h> <m> <m> <m> <h> <h> <|sep|>': ['同一片天空何时会放晴', '同一片天空何时会出现', '同一片天空同一个梦境', '同一片天空同一个方向', '同一片天空何时会变晴']}\n",
      "{'input_ids': tensor([[     6,   1801,  12392, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250027,\n",
      "         250027, 250030, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   1801,  12392, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250027,\n",
      "         250027, 250030, 250027, 250031,      2, 250025]])\n",
      "{'心痛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <h> <h> <m> <h> <|sep|>': ['连心痛亦不痛不痒痒', '连心痛痛快淋漓尽致', '连心痛亦是不痛实际', '连心痛痛快淋湿肝肠', '连心痛痛快淋湿了']}\n",
      "{'input_ids': tensor([[     6,   5477, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250027, 250027, 250027,\n",
      "         250030, 250030, 250030, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   5477, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335,   4426,  34145,     42,   2740, 250030, 250027, 250027, 250027,\n",
      "         250030, 250030, 250030, 250027, 250027, 250031,      2, 250025]])\n",
      "{'时间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <h> <h> <m> <m> <m> <h> <h> <|sep|>': ['时间一去不复返', '时间一分一秒一分一秒', '时间一去不复再返', '时间一去不复再见', '时间一分一秒一滴滴答']}\n",
      "{'input_ids': tensor([[     6,  13599, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250027, 250030,\n",
      "         250030, 250027, 250028, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  13599, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250027, 250030,\n",
      "         250030, 250027, 250028, 250028, 250031,      2, 250025]])\n",
      "{'人生 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <m> <h> <l> <l> <|sep|>': ['笑看人生苦短有滋', '笑看人生苦短有苦有', '笑看人生百转百回', '笑看人生百转千回', '笑看人生苦短有苦有短']}\n",
      "{'input_ids': tensor([[     6,  17959,   1801, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250028,\n",
      "         250027, 250028, 250028, 250030, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  17959,   1801, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250028,\n",
      "         250027, 250028, 250028, 250030, 250030, 250031,      2, 250025]])\n",
      "{'伤心 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b1> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <m> <m> <|sep|>': ['别忘了伤脑筋竭而尽', '别忘了伤脑筋竭竭', '别忘了伤脑筋急男儿', '缠绕着伤脑筋竭而尽', '别忘了伤有好长长']}\n",
      "{'input_ids': tensor([[     6,   7558,    487, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250336, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335,   4426,  34145,     42,   2740, 250030, 250027,\n",
      "         250028, 250027, 250028, 250028, 250027, 250027, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   7558,    487, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250336, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335,   4426,  34145,     42,   2740, 250030, 250027,\n",
      "         250028, 250027, 250028, 250028, 250027, 250027, 250031,      2, 250025]])\n",
      "{'爱人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <h> <h> <|sep|>': ['不是每分每秒钟爱人', '不是我爱你你会爱人', '不是每分每秒在爱人', '陪伴我爱你永不分离', '陪伴我千百千万万']}\n",
      "{'input_ids': tensor([[     6,  29476, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250336, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250028,\n",
      "         250027, 250028, 250028, 250030, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  29476, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250335, 250336, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250030, 250027, 250028,\n",
      "         250027, 250028, 250028, 250030, 250030, 250031,      2, 250025]])\n",
      "{'感情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <m> <m> <|sep|>': ['不是每段感情绵绵', '还是有说有有笑有感情', '还是我爱你感情薄', '还是两厢有感情无底', '还是有说有有笑有']}\n",
      "{'input_ids': tensor([[     6,  84460, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335,   4426,  34145,     42,   2740, 250027,\n",
      "         250027, 250030, 250028, 250027, 250027, 250030, 250028, 250027, 250030,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  84460, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250336, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335,   4426,  34145,     42,   2740, 250027,\n",
      "         250027, 250030, 250028, 250027, 250027, 250030, 250028, 250027, 250030,\n",
      "         250031,      2, 250025]])\n",
      "{'记忆 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <l> <h> <h> <m> <l> <h> <m> <|sep|>': ['记忆无可救药无可救人', '记忆无可救药无法自拔', '记忆无所事事无所事', '记忆无法控制得了爱和恨', '记忆无所事事无所事无']}\n",
      "{'input_ids': tensor([[     6,  41029, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,   2740,\n",
      "         250027, 250030, 250028, 250030, 250027, 250028, 250030, 250028, 250028,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  41029, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250336,   4426,  34145,     42,   2740,\n",
      "         250027, 250030, 250028, 250030, 250027, 250028, 250030, 250028, 250028,\n",
      "         250031,      2, 250025]])\n",
      "{'习惯 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <h> <l> <m> <l> <l> <|sep|>': ['摸爬滚龙去往无处可逃', '摸爬滚龙去早来早往', '摸爬滚龙去早来早往习惯', '摸爬滚龙去早来早往惯', '摸爬滚龙去早成了习惯']}\n",
      "{'input_ids': tensor([[     6, 162253, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335,   4426,  34145,     42,   2740, 250030,\n",
      "         250030, 250028, 250027, 250030, 250030, 250030, 250030, 250030, 250031,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 162253, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335,   4426,  34145,     42,   2740, 250030,\n",
      "         250030, 250028, 250027, 250030, 250030, 250030, 250030, 250030, 250031,\n",
      "              2, 250025]])\n",
      "{'滋味 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <l> <h> <m> <m> <m> <m> <m> <|sep|>': ['尝尝苦辣甜甜甜甜滋味', '甜甜苦辣咸咸咸滋味', '甜甜苦辣咸咸咸咸', '茫茫苦中尝尝尝滋味', '愁肠百千愁肠百回滋味']}\n",
      "{'input_ids': tensor([[     6, 142270, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250335,   4426,  34145,     42,   2740,\n",
      "         250027, 250027, 250030, 250030, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 142270, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335, 250335, 250335, 250335,   4426,  34145,     42,   2740,\n",
      "         250027, 250027, 250030, 250030, 250030, 250030, 250030, 250030, 250030,\n",
      "         250028, 250031,      2, 250025]])\n",
      "{'情人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <m> <m> <m> <m> <m> <m> <l> <|sep|>': ['梦中情人迷朦朦胧胧', '梦中情人迷朦朦糊糊糊', '梦中情人迷朦胧胧如影', '岁岁年年情人节好吗', '梦中情人迷朦朦糊糊']}\n",
      "{'input_ids': tensor([[     6, 178203, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250045, 250032, 250335, 250335, 250336,\n",
      "         250335, 250336, 250335, 250335, 250335, 250335, 250336,   4426,  34145,\n",
      "             42,   2740, 250030, 250030, 250030, 250027, 250027, 250027, 250027,\n",
      "         250027, 250027, 250030, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 178203, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250045, 250032, 250335, 250335, 250336,\n",
      "         250335, 250336, 250335, 250335, 250335, 250335, 250336,   4426,  34145,\n",
      "             42,   2740, 250030, 250030, 250030, 250027, 250027, 250027, 250027,\n",
      "         250027, 250027, 250030, 250030, 250031,      2, 250025]])\n",
      "{'勇气 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <|num|> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <m> <m> <h> <h> <h> <h> <h> <h> <m> <m> <|sep|>': ['何时能再次失去重来回勇气', '何时能再次失去重重回旋', '没人能代替那份份离和勇气', '何时能再见那份重回勇气', '没人能代替那份重重回勇气']}\n",
      "{'input_ids': tensor([[     6,  17246, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,\n",
      "             42,   2740, 250030, 250028, 250030, 250027, 250027, 250030, 250027,\n",
      "         250030, 250027, 250028, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  17246, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250335,\n",
      "         250336, 250335, 250335, 250335, 250335, 250335, 250336,   4426,  34145,\n",
      "             42,   2740, 250030, 250028, 250030, 250027, 250027, 250030, 250027,\n",
      "         250030, 250027, 250028, 250031,      2, 250025]])\n",
      "{'故事 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <m> <h> <m> <h> <l> <|sep|>': ['别以为故事来之不易', '唐古拉多半唐宋明清往今', '回首来时路一幕幕故事', '唐古拉多奇唐诗宋故事', '回首来时路一幕幕故事涌现']}\n",
      "{'input_ids': tensor([[     6,  63112, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250336,\n",
      "         250335, 250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250030, 250028, 250030, 250027, 250027, 250030, 250027, 250030,\n",
      "         250027, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  63112, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250335, 250335, 250335, 250336,\n",
      "         250335, 250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,\n",
      "           2740, 250030, 250028, 250030, 250027, 250027, 250030, 250027, 250030,\n",
      "         250027, 250030, 250031,      2, 250025]])\n",
      "{'青春 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <m> <h> <m> <h> <m> <|sep|>': ['流血流汗换来青春年华', '流血流汗换来青春年少无悔', '流血流汗拼搏尽全力青春', '流血流汗换来金银青春年少', '流血流汗换来金银青春无悔']}\n",
      "{'input_ids': tensor([[     6, 112895, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250336, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250028, 250030, 250027, 250027, 250027, 250027, 250027, 250030,\n",
      "         250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 112895, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250335, 250335, 250335, 250336, 250335,\n",
      "         250336, 250335, 250335, 250336, 250335,   4426,  34145,     42,   2740,\n",
      "         250030, 250028, 250030, 250027, 250027, 250027, 250027, 250027, 250030,\n",
      "         250031,      2, 250025]])\n",
      "{'美丽 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <h> <m> <|sep|>': ['无与伦比绽放出美丽来临', '形影不离这份爱爱美丽', '无与伦比绽放一丝丝美丽', '无与伦比美丽一万年长', '无与伦比绽放出美丽来']}\n",
      "{'input_ids': tensor([[     6, 119715, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250030, 250027,\n",
      "         250027, 250027, 250028, 250030, 250031,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 119715, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250335, 250335, 250335, 250335, 250335, 250335, 250335,\n",
      "         250335, 250335,   4426,  34145,     42,   2740, 250027, 250030, 250027,\n",
      "         250027, 250027, 250028, 250030, 250031,      2, 250025]])\n",
      "{'命运 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <m> <h> <h> <h> <l> <m> <|sep|>': ['顺其自然命运起伏伏', '任劳任怨命运扭头颅', '任劳任怨命运扭头', '任劳任怨命运主宰', '任劳任怨命运起伏伏']}\n"
     ]
    }
   ],
   "source": [
    "three_height_generation = list()\n",
    "\n",
    "for i in range(len(prompts_wb)):\n",
    "    word_list = [prompts_wb[i]]\n",
    "    emb_list = []\n",
    "    for item in word_list:\n",
    "        emb_list.append({'translation': {'hi': item,'en': ''}})\n",
    "    A = data_collator(emb_list)\n",
    "    print(A)\n",
    "    print(A['input_ids'])\n",
    "\n",
    "    fea_output = {}\n",
    "    for i in range(A['input_ids'].shape[0]):\n",
    "        B = torch.unsqueeze(A['input_ids'][i],0)\n",
    "        output = mbart_model.generate(B.to(device='cuda'), max_length=500,num_beams=5, num_return_sequences=5)\n",
    "        fea_output[word_list[i]] = tokenizer.batch_decode(output, skip_special_tokens=True,temperature=0.5)\n",
    "    #     print(word_list[i],tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "        print(fea_output)\n",
    "    three_height_generation.append(fea_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8330e68c-a40e-4a4e-96a0-f3373f1ecc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'无情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <h> <h> <|sep|>': ['无语无泪爱上毒',\n",
       "   '无语无声爱上毒',\n",
       "   '无语无声爱上你',\n",
       "   '无语无泪爱不义',\n",
       "   '无语无泪心花瓣']},\n",
       " {'泪水 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <m> <h> <l> <|sep|>': ['流血流汗流泪水滴',\n",
       "   '难舍难分流泪水淌',\n",
       "   '无悔无怨流泪水滴',\n",
       "   '无法平息流泪水',\n",
       "   '好久不见流泪水']},\n",
       " {'眼神 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <l> <m> <|sep|>': ['含苞欲放迷了眼神',\n",
       "   '不屑一顾谁的眼神',\n",
       "   '含苞欲放红眼神',\n",
       "   '不屑一顾无眼神',\n",
       "   '含苞欲放红眼神迷']},\n",
       " {'风雨 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b0> <b0> <bdr> <h> <m> <m> <m> <m> <l> <l> <l> <m> <l> <|sep|>': ['风雨来来往往我无所畏',\n",
       "   '任凭寒来暑往我们迎着风',\n",
       "   '任凭来来回回的有雨有风',\n",
       "   '任凭来来回回的有无有',\n",
       "   '任凭来来回回的有雨淋着风']},\n",
       " {'结果 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <h> <m> <m> <m> <m> <l> <l> <h> <l> <l> <|sep|>': ['到头来来结果三百九十九',\n",
       "   '到头来来结果一场赌注',\n",
       "   '到头来来结果三百五十分',\n",
       "   '到头来来结果三百五千',\n",
       "   '到头来来结果三九九九']},\n",
       " {'心情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <m> <l> <m> <h> <m> <|sep|>': ['离家出没来久而心情烦',\n",
       "   '年复一年烦杂人心情烦',\n",
       "   '年复一年烦杂人心情',\n",
       "   '离家出没来久别心情烦',\n",
       "   '离家出没来起伏心情起伏']},\n",
       " {'朋友 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <h> <h> <|sep|>': ['朋友难聚聚聚聚聚散散',\n",
       "   '朋友来作伴肩并肩作战',\n",
       "   '朋友来作伴肩并肩站稳',\n",
       "   '朋友和睦睦笑一笑开怀',\n",
       "   '朋友难聚聚聚聚散散聚']},\n",
       " {'思念 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <h> <m> <l> <m> <m> <m> <m> <m> <l> <l> <|sep|>': ['蓦然回眸来来回回的思念',\n",
       "   '蓦然回眸茫茫人海里头',\n",
       "   '蓦然回眸朦朦胧胧想念',\n",
       "   '蓦然回眸朦朦胧胧思念',\n",
       "   '蓦然回眸茫茫人海里思']},\n",
       " {'日子 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <m> <h> <m> <h> <h> <h> <|sep|>': ['祝福祖国年少无知日子',\n",
       "   '多年以前同甘共命运日子',\n",
       "   '祝福祖国和衷共济日子',\n",
       "   '祝福祖国繁荣富强日子',\n",
       "   '祝福祖国平淡无味日子']},\n",
       " {'世界 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <m> <m> <m> <m> <l> <l> <|sep|>': ['既然选择环游世界的五彩',\n",
       "   '既然选择环游世界的顶端',\n",
       "   '终于醒来原来如此而已',\n",
       "   '祝福祖国繁荣民主民主',\n",
       "   '未来属于贫穷穷国家的']},\n",
       " {'时光 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <m> <h> <l> <m> <m> <l> <l> <|sep|>': ['随时光转红红火火',\n",
       "   '随时光转来来往往',\n",
       "   '原来真的时光荏苒苒苒',\n",
       "   '随时光转十百转百转',\n",
       "   '随时光转来来来往往']},\n",
       " {'女人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b1> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <m> <|sep|>': ['男女人真心真意疼惜',\n",
       "   '男女人一去不复回首',\n",
       "   '男女人真心真意疼',\n",
       "   '男女人自甘堕落无情',\n",
       "   '男女人一去不复回']},\n",
       " {'笑容 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <h> <m> <l> <|sep|>': ['面带笑容泪如雨下',\n",
       "   '面带笑容义无反顾',\n",
       "   '面带笑容泪流满面',\n",
       "   '面带笑容泪含着雨',\n",
       "   '面带笑容涕红了眼']},\n",
       " {'爱情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <m> <h> <h> <h> <h> <m> <|sep|>': ['谈何容易见真爱情',\n",
       "   '谈何容易失去爱情',\n",
       "   '谈谈情说爱爱情',\n",
       "   '谈何容易见怪爱情',\n",
       "   '谈谈情说说爱情']},\n",
       " {'声音 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <l> <m> <h> <h> <h> <|sep|>': ['好久不见声音沙哑',\n",
       "   '潮起潮落声音嘶哑',\n",
       "   '来来回去声音嘶哑',\n",
       "   '十指连心花怒放声音',\n",
       "   '来来回回声音嘶哑']},\n",
       " {'感觉 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <l> <l> <m> <h> <h> <|sep|>': ['冥冥之中有种奇妙',\n",
       "   '甜甜蜜蜜的感觉',\n",
       "   '情不自禁有种奇妙',\n",
       "   '寻寻觅觅总有极限期',\n",
       "   '寻寻觅觅总找不到家']},\n",
       " {'流泪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <l> <h> <h> <h> <h> <m> <|sep|>': ['流流泪泪喜怒哀乐无常',\n",
       "   '流年似箭矢志不渝流泪',\n",
       "   '流流泪泪洗面具具足矣',\n",
       "   '流年似箭矢志不屈为流泪',\n",
       "   '流流泪泪洗面具具具足矣']},\n",
       " {'时候 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b1> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <m> <h> <h> <l> <l> <m> <h> <h> <|sep|>': ['明明知道我们没说出时候',\n",
       "   '明明知道我们来时时候',\n",
       "   '明明知道我们没说个时候',\n",
       "   '明明知道我们没关系时候',\n",
       "   '临别时候我们绝不会分手']},\n",
       " {'眼睛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <h> <h> <m> <h> <|sep|>': ['迷迷糊糊睁不开眼睛',\n",
       "   '迷迷糊糊蒙蒙眼睛',\n",
       "   '朦朦胧胧睁不开眼睛',\n",
       "   '迷迷糊糊糊蒙上眼睛',\n",
       "   '红着眼睛不吭不吭']},\n",
       " {'梦想 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <m> <l> <m> <h> <h> <h> <h> <h> <|sep|>': ['追寻所求之光为梦想',\n",
       "   '欢迎你来加入为梦想',\n",
       "   '追寻所求之光放梦想',\n",
       "   '追寻所求之光放飞梦想',\n",
       "   '追寻理想乘风破浪为梦想']},\n",
       " {'世间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <h> <m> <l> <m> <h> <h> <h> <h> <h> <|sep|>': ['再回首来生看世间变迁',\n",
       "   '再回首来生看世间沧桑',\n",
       "   '再回首来时看世间变迁',\n",
       "   '酸甜苦辣看世间变迁',\n",
       "   '再回首来生看世间苍老']},\n",
       " {'地方 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <m> <m> <m> <h> <l> <|sep|>': ['无论在何时同一个地方',\n",
       "   '无论到何时同一个地方',\n",
       "   '不论到何时停不了地方',\n",
       "   '无论到何时停不了地方',\n",
       "   '不论到何时同一个地方']},\n",
       " {'流浪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <m> <m> <h> <h> <h> <|sep|>': ['茫茫人海中漂荡',\n",
       "   '何时何地流浪街头',\n",
       "   '茫茫人海中爱恨情',\n",
       "   '茫茫人海中爱上流浪',\n",
       "   '茫茫人海中之道者']},\n",
       " {'天空 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b1> <b0> <bdr> <m> <h> <h> <h> <m> <m> <m> <h> <h> <|sep|>': ['同一片天空何时会放晴',\n",
       "   '同一片天空何时会出现',\n",
       "   '同一片天空同一个梦境',\n",
       "   '同一片天空同一个方向',\n",
       "   '同一片天空何时会变晴']},\n",
       " {'心痛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <h> <h> <m> <h> <|sep|>': ['连心痛亦不痛不痒痒',\n",
       "   '连心痛痛快淋漓尽致',\n",
       "   '连心痛亦是不痛实际',\n",
       "   '连心痛痛快淋湿肝肠',\n",
       "   '连心痛痛快淋湿了']},\n",
       " {'时间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <h> <h> <m> <m> <m> <h> <h> <|sep|>': ['时间一去不复返',\n",
       "   '时间一分一秒一分一秒',\n",
       "   '时间一去不复再返',\n",
       "   '时间一去不复再见',\n",
       "   '时间一分一秒一滴滴答']},\n",
       " {'人生 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <m> <h> <l> <l> <|sep|>': ['笑看人生苦短有滋',\n",
       "   '笑看人生苦短有苦有',\n",
       "   '笑看人生百转百回',\n",
       "   '笑看人生百转千回',\n",
       "   '笑看人生苦短有苦有短']},\n",
       " {'伤心 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b0> <b1> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <m> <m> <|sep|>': ['别忘了伤脑筋竭而尽',\n",
       "   '别忘了伤脑筋竭竭',\n",
       "   '别忘了伤脑筋急男儿',\n",
       "   '缠绕着伤脑筋竭而尽',\n",
       "   '别忘了伤有好长长']},\n",
       " {'爱人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <h> <h> <|sep|>': ['不是每分每秒钟爱人',\n",
       "   '不是我爱你你会爱人',\n",
       "   '不是每分每秒在爱人',\n",
       "   '陪伴我爱你永不分离',\n",
       "   '陪伴我千百千万万']},\n",
       " {'感情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <h> <l> <h> <l> <l> <m> <m> <|sep|>': ['不是每段感情绵绵',\n",
       "   '还是有说有有笑有感情',\n",
       "   '还是我爱你感情薄',\n",
       "   '还是两厢有感情无底',\n",
       "   '还是有说有有笑有']},\n",
       " {'记忆 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <l> <h> <h> <m> <l> <h> <m> <|sep|>': ['记忆无可救药无可救人',\n",
       "   '记忆无可救药无法自拔',\n",
       "   '记忆无所事事无所事',\n",
       "   '记忆无法控制得了爱和恨',\n",
       "   '记忆无所事事无所事无']},\n",
       " {'习惯 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <h> <m> <l> <m> <h> <l> <m> <l> <l> <|sep|>': ['摸爬滚龙去往无处可逃',\n",
       "   '摸爬滚龙去早来早往',\n",
       "   '摸爬滚龙去早来早往习惯',\n",
       "   '摸爬滚龙去早来早往惯',\n",
       "   '摸爬滚龙去早成了习惯']},\n",
       " {'滋味 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <m> <m> <l> <h> <m> <m> <m> <m> <m> <|sep|>': ['尝尝苦辣甜甜甜甜滋味',\n",
       "   '甜甜苦辣咸咸咸滋味',\n",
       "   '甜甜苦辣咸咸咸咸',\n",
       "   '茫茫苦中尝尝尝滋味',\n",
       "   '愁肠百千愁肠百回滋味']},\n",
       " {'情人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <h> <m> <m> <m> <m> <m> <m> <m> <l> <|sep|>': ['梦中情人迷朦朦胧胧',\n",
       "   '梦中情人迷朦朦糊糊糊',\n",
       "   '梦中情人迷朦胧胧如影',\n",
       "   '岁岁年年情人节好吗',\n",
       "   '梦中情人迷朦朦糊糊']},\n",
       " {'勇气 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <|num|> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <m> <m> <h> <h> <h> <h> <h> <h> <m> <m> <|sep|>': ['何时能再次失去重来回勇气',\n",
       "   '何时能再次失去重重回旋',\n",
       "   '没人能代替那份份离和勇气',\n",
       "   '何时能再见那份重回勇气',\n",
       "   '没人能代替那份重重回勇气']},\n",
       " {'故事 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b0> <b1> <b0> <b0> <b0> <b0> <b0> <b1> <bdr> <m> <l> <m> <h> <h> <m> <h> <m> <h> <l> <|sep|>': ['别以为故事来之不易',\n",
       "   '唐古拉多半唐宋明清往今',\n",
       "   '回首来时路一幕幕故事',\n",
       "   '唐古拉多奇唐诗宋故事',\n",
       "   '回首来时路一幕幕故事涌现']},\n",
       " {'青春 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <m> <h> <m> <h> <m> <|sep|>': ['流血流汗换来青春年华',\n",
       "   '流血流汗换来青春年少无悔',\n",
       "   '流血流汗拼搏尽全力青春',\n",
       "   '流血流汗换来金银青春年少',\n",
       "   '流血流汗换来金银青春无悔']},\n",
       " {'美丽 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <b0> <b0> <b0> <b1> <b0> <b1> <b0> <b0> <b1> <b0> <bdr> <m> <l> <m> <h> <h> <h> <h> <h> <m> <|sep|>': ['无与伦比绽放出美丽来临',\n",
       "   '形影不离这份爱爱美丽',\n",
       "   '无与伦比绽放一丝丝美丽',\n",
       "   '无与伦比美丽一万年长',\n",
       "   '无与伦比绽放出美丽来']},\n",
       " {'命运 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <b0> <bdr> <h> <m> <h> <h> <h> <l> <m> <|sep|>': ['顺其自然命运起伏伏',\n",
       "   '任劳任怨命运扭头颅',\n",
       "   '任劳任怨命运扭头',\n",
       "   '任劳任怨命运主宰',\n",
       "   '任劳任怨命运起伏伏']}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_height_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd78a94-8b94-4934-a077-1199ad9732c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_height_generation = list()\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    word_list = [prompts[i]]\n",
    "    emb_list = []\n",
    "    for item in word_list:\n",
    "        emb_list.append({'translation': {'hi': item,'en': ''}})\n",
    "    A = data_collator(emb_list)\n",
    "    print(A)\n",
    "    print(A['input_ids'])\n",
    "\n",
    "    fea_output = {}\n",
    "    for i in range(A['input_ids'].shape[0]):\n",
    "        B = torch.unsqueeze(A['input_ids'][i],0)\n",
    "        output = mbart_model.generate(B.to(device='cuda'), max_length=500,num_beams=5, num_return_sequences=5)\n",
    "        fea_output[word_list[i]] = tokenizer.batch_decode(output, skip_special_tokens=True,temperature=0.5)\n",
    "    #     print(word_list[i],tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "        print(fea_output)\n",
    "    three_height_generation.append(fea_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea6c14-7c6c-4762-a02d-37b59787da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_idx_keywords = dict()\n",
    "with open(\"/home/yc429/Ch_lyrics_generation/12_new_melodies/keywords_random_12_group.txt\",\"r\") as fin:\n",
    "    idx = 1\n",
    "    for line in fin.readlines()[1:]:\n",
    "        line = line.strip()\n",
    "        keywords = line.split(\"\\t\")[0].split()\n",
    "        midi_idx_keywords[idx] = \",\".join(keywords)\n",
    "        idx += 1\n",
    "\n",
    "def swap_keys_values(dictionary):\n",
    "    swapped_dict = {value: key for key, value in dictionary.items()}\n",
    "    return swapped_dict\n",
    "\n",
    "keywords2midi = swap_keys_values(midi_idx_keywords)\n",
    "print(keywords2midi)\n",
    "\n",
    "\n",
    "\n",
    "for x in three_height_generation:\n",
    "    keyword = list(x.keys())[0].split(\" <|kwd|> \")[0]\n",
    "    for key in keywords2midi.keys():\n",
    "        if keyword in key:\n",
    "            idx = keywords2midi[key]\n",
    "    print(idx, keyword,list(x.values())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c9db37-fd26-4158-bc76-eb3febe77ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'three_height_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mthree_height_generation\u001b[49m:\n\u001b[1;32m      2\u001b[0m     keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <kwd> \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keywords2midi\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'three_height_generation' is not defined"
     ]
    }
   ],
   "source": [
    "for x in three_height_generation:\n",
    "    keyword = list(x.keys())[0].split(\" <kwd> \")[0]\n",
    "    for key in keywords2midi.keys():\n",
    "        if keyword in key:\n",
    "            idx = keywords2midi[key]\n",
    "    print(idx, keyword,list(x.values())[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864e819-fead-4956-9738-65a843b2b955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in three_height_generation:\n",
    "    keyword = list(x.keys())[0].split(\" <kwd> \")[0]\n",
    "    for key in keywords2midi.keys():\n",
    "        if keyword in key:\n",
    "            idx = keywords2midi[key]\n",
    "    print(idx, keyword,list(x.values())[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53e1cfff-845c-4bdb-9430-5d2c68786484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     6, 229051,   1344, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250336, 250335, 250336, 250335,\n",
      "         250336, 250335, 250336,   4426,  34145,     42,   2740, 250027, 250030,\n",
      "         250027, 250027, 250030, 250030, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 229051,   1344, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250335, 250336, 250335, 250336, 250335,\n",
      "         250336, 250335, 250336,   4426,  34145,     42,   2740, 250027, 250030,\n",
      "         250027, 250027, 250030, 250030, 250027, 250027,      2, 250025]])\n",
      "{'笛子 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b1> <b0> <b1> <b0> <b1> <bdr> <h> <m> <h> <h> <m> <m> <h> <h> ': ['太阳落山笛儿悠悠', '这时一只笛子吹过', '太阳落山笛儿吹过', '忽然听到笛子吹过', '这时一只笛子吹奏']}\n"
     ]
    }
   ],
   "source": [
    "mbart_model.eval()\n",
    "\n",
    "word_list = [\"笛子 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <b0> <b1> <b0> <b1> <b0> <b1> <b0> <b1> <bdr> <h> <m> <h> <h> <m> <m> <h> <h> \"]\n",
    "emb_list = []\n",
    "for item in word_list:\n",
    "    emb_list.append({'translation': {'hi': item,'en': ''}})\n",
    "A = data_collator(emb_list)\n",
    "print(A)\n",
    "print(A['input_ids'])\n",
    "\n",
    "fea_output = {}\n",
    "for i in range(A['input_ids'].shape[0]):\n",
    "    B = torch.unsqueeze(A['input_ids'][i],0)\n",
    "    output = mbart_model.generate(B.to(device='cuda'), max_length=500,num_beams=5, num_return_sequences=5)\n",
    "    fea_output[word_list[i]] = tokenizer.batch_decode(output, skip_special_tokens=True,temperature=0.5)\n",
    "#     print(word_list[i],tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "    print(fea_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77141f0e-8fa5-4b07-a2cb-0a659d59a008",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './mbart_3_height_epoch3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mbart_3_height_epoch3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Find all subdirectories of output_dir.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m subdirs \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, d))]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Sort them by last modification time.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m subdirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(subdirs, key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './mbart_3_height_epoch3'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's assume this is your output directory.\n",
    "output_dir = \"./mbart_3_height_epoch3\"\n",
    "\n",
    "# Find all subdirectories of output_dir.\n",
    "subdirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "# Sort them by last modification time.\n",
    "subdirs = sorted(subdirs, key=os.path.getmtime)\n",
    "\n",
    "# The last checkpoint is the most recent one.\n",
    "last_checkpoint = subdirs[-1]\n",
    "\n",
    "print(last_checkpoint)\n",
    "\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mbart_model.resize_token_embeddings(len(tokenizer))\n",
    "mbart_model.to(device)\n",
    "mbart_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a22be9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./mbart_4_height_epoch3/checkpoint-57000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): Embedding(250339, 1024)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250339, 1024)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250339, 1024)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250339, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's assume this is your output directory.\n",
    "output_dir = \"./mbart_4_height_epoch3\"\n",
    "\n",
    "# Find all subdirectories of output_dir.\n",
    "subdirs = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "\n",
    "# Sort them by last modification time.\n",
    "subdirs = sorted(subdirs, key=os.path.getmtime)\n",
    "\n",
    "# The last checkpoint is the most recent one.\n",
    "last_checkpoint = subdirs[-1]\n",
    "\n",
    "print(last_checkpoint)\n",
    "\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mbart_model.resize_token_embeddings(len(tokenizer))\n",
    "mbart_model.to(device)\n",
    "mbart_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "610d7438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./mbart_4_height_epoch3/checkpoint-57000).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MBartForConditionalGeneration:\n\tsize mismatch for final_logits_bias: copying a param with shape torch.Size([1, 250349]) from checkpoint, the shape in current model is torch.Size([1, 250339]).\n\tsize mismatch for model.shared.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for model.encoder.embed_tokens.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for model.decoder.embed_tokens.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmbart_model,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Start fine-tuning\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/trainer.py:1108\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(resume_from_checkpoint, WEIGHTS_NAME), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;66;03m# release memory\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/trainer.py:1481\u001b[0m, in \u001b[0;36mTrainer._load_state_dict_in_model\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_state_dict_in_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_dict):\n\u001b[0;32m-> 1481\u001b[0m     load_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(load_result\u001b[38;5;241m.\u001b[39mmissing_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m(load_result\u001b[38;5;241m.\u001b[39mmissing_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m   1485\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_save\n\u001b[1;32m   1486\u001b[0m         ):\n",
      "File \u001b[0;32m~/.conda/envs/gpt2/lib/python3.8/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MBartForConditionalGeneration:\n\tsize mismatch for final_logits_bias: copying a param with shape torch.Size([1, 250349]) from checkpoint, the shape in current model is torch.Size([1, 250339]).\n\tsize mismatch for model.shared.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for model.encoder.embed_tokens.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for model.decoder.embed_tokens.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([250349, 1024]) from checkpoint, the shape in current model is torch.Size([250339, 1024])."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_1017_epoch3/checkpoint-36000\n",
      "Configuration saved in ./mbart_3_height_1017_epoch3/checkpoint-36000/config.json\n",
      "Model weights saved in ./mbart_3_height_1017_epoch3/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_1017_epoch3/checkpoint-31000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_1017_epoch3/checkpoint-37000\n",
      "Configuration saved in ./mbart_3_height_1017_epoch3/checkpoint-37000/config.json\n",
      "Model weights saved in ./mbart_3_height_1017_epoch3/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_1017_epoch3/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_1017_epoch3/checkpoint-38000\n",
      "Configuration saved in ./mbart_3_height_1017_epoch3/checkpoint-38000/config.json\n",
      "Model weights saved in ./mbart_3_height_1017_epoch3/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_1017_epoch3/checkpoint-33000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_1017_epoch3/checkpoint-39000\n",
      "Configuration saved in ./mbart_3_height_1017_epoch3/checkpoint-39000/config.json\n",
      "Model weights saved in ./mbart_3_height_1017_epoch3/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_1017_epoch3/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./mbart_3_height_1017_epoch3/checkpoint-40000\n",
      "Configuration saved in ./mbart_3_height_1017_epoch3/checkpoint-40000/config.json\n",
      "Model weights saved in ./mbart_3_height_1017_epoch3/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [mbart_3_height_1017_epoch3/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65274\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_3_height_1017_epoch3\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "# Create the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # You can define a data collator if needed\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    "    \n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train(last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff52d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbart_model.save_pretrained('./3_height_epoch_3_1017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f155adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     6,  68132, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250028, 250027, 250030, 250027, 250030, 250027, 250028,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  68132, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250028, 250027, 250030, 250027, 250030, 250027, 250028,\n",
      "              2, 250025]])\n",
      "{'夏天 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <l> <h> <m> <h> <m> <h> <l> ': ['把爱留在夏天里', '把爱埋在夏天里', '把爱埋进夏天里', '想念随季节变冷', '想念如夏天的雨']}\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"夏天 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <l> <h> <m> <h> <m> <h> <l> \"]\n",
    "emb_list = []\n",
    "for item in word_list:\n",
    "    emb_list.append({'translation': {'hi': item,'en': ''}})\n",
    "A = data_collator(emb_list)\n",
    "print(A)\n",
    "print(A['input_ids'])\n",
    "\n",
    "fea_output = {}\n",
    "for i in range(A['input_ids'].shape[0]):\n",
    "    B = torch.unsqueeze(A['input_ids'][i],0)\n",
    "    output = mbart_model.generate(B.to(device='cuda'), max_length=500,num_beams=5, num_return_sequences=5)\n",
    "    fea_output[word_list[i]] = tokenizer.batch_decode(output, skip_special_tokens=True,temperature=0.5)\n",
    "#     print(word_list[i],tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "    print(fea_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7444ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbart_model.eval()\n",
    "prompts = list()\n",
    "with open(\"./M-scansion_inference_10_melody_prompts/4_height_similarity_10_melody_prompts.txt\",\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        prompts.append(\" \".join(line.strip().split(\"\\t\")[-1].split()[:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a710764-e2d1-432e-a4d7-7cc77a99df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'黑夜,无情,泪水,眼神': 1, '风雨,结果,心情,朋友': 2, '思念,日子,世界,时光': 3, '女人,笑容,爱情,声音': 4, '感觉,流泪,时候,眼睛': 5, '梦想,世间,地方,流浪': 6, '天空,心痛,时间,人生': 7, '伤心,爱人,感情,记忆': 8, '习惯,滋味,情人,勇气': 9, '故事,青春,美丽,命运': 10}\n"
     ]
    }
   ],
   "source": [
    "midi2keywords = {1: '黑夜,无情,泪水,眼神',\n",
    " 2: '风雨,结果,心情,朋友',\n",
    " 3: '思念,日子,世界,时光',\n",
    " 4: '女人,笑容,爱情,声音',\n",
    " 5: '感觉,流泪,时候,眼睛',\n",
    " 6: '梦想,世间,地方,流浪',\n",
    " 7: '天空,心痛,时间,人生',\n",
    " 8: '伤心,爱人,感情,记忆',\n",
    " 9: '习惯,滋味,情人,勇气',\n",
    " 10: '故事,青春,美丽,命运'}\n",
    "\n",
    "def swap_keys_values(dictionary):\n",
    "    swapped_dict = {value: key for key, value in dictionary.items()}\n",
    "    return swapped_dict\n",
    "\n",
    "keywords2midi = swap_keys_values(midi2keywords)\n",
    "print(keywords2midi)\n",
    "\n",
    "def output_lyrics(prompt,model):\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "    sample_outputs = model.generate(\n",
    "                            generated, \n",
    "                            #bos_token_id=random.randint(1,30000),\n",
    "                            do_sample=True,   \n",
    "                            top_k=10, \n",
    "                            max_length = 300,\n",
    "                            top_p=0.95, \n",
    "                            num_return_sequences=1\n",
    "                            )\n",
    "\n",
    "    res = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5416022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc429/.conda/envs/gpt2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3415: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     6,   7320,   8097, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250028, 250029, 250028, 250027, 250030, 250028,\n",
      "         250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   7320,   8097, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250028, 250029, 250028, 250027, 250030, 250028,\n",
      "         250030,      2, 250025]])\n",
      "{'黑夜 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <l> <d> <l> <h> <m> <l> <m>': ['守着你黑夜很长', '守着你黑夜两茫', '有了你黑夜远离', '守着你黑夜旅行', '守着你黑夜两头']}\n",
      "{'input_ids': tensor([[     6,   3759,   5148, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250028, 250029, 250028, 250027, 250030, 250027,\n",
      "         250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   3759,   5148, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250032, 250028, 250029, 250028, 250027, 250030, 250027,\n",
      "         250027,      2, 250025]])\n",
      "{'无情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <l> <d> <l> <h> <m> <h> <h>': ['你的每句无情话', '给了你无情伤害', '你的每句无情句', '你的美丽如花瓣', '你的每句情和义']}\n",
      "{'input_ids': tensor([[     6, 134479,   1553, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250029, 250028, 250027, 250030,\n",
      "         250028, 250030, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 134479,   1553, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250029, 250028, 250027, 250030,\n",
      "         250028, 250030, 250029,      2, 250025]])\n",
      "{'泪水 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <d> <l> <h> <m> <l> <m> <d>': ['我的眼中泪水流了', '我的眼中泪水流着', '你的眼中泪水流着', '你的眼中泪水流了', '我的眼泪流给谁了']}\n",
      "{'input_ids': tensor([[     6,   5003,   4183, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250032, 250028, 250030, 250030, 250029, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   5003,   4183, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250032, 250028, 250030, 250030, 250029, 250028,      2, 250025]])\n",
      "{'眼神 <|kwd|> <c0> <c1> <c2> <c3> <c4> <|num|> <l> <m> <m> <d> <l>': ['眼神陪着我', '眼神陪着你', '眼神迷了眼', '眼神随着你', '眼神连着我']}\n",
      "{'input_ids': tensor([[     6,   7149,   9731, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250027, 250030, 250030,\n",
      "         250030, 250030, 250029, 250029, 250029, 250030, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   7149,   9731, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250027, 250030, 250030,\n",
      "         250030, 250030, 250029, 250029, 250029, 250030, 250028,      2, 250025]])\n",
      "{'风雨 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <m> <m> <m> <m> <d> <d> <d> <m> <l>': ['风雨来临时啊啊啊啊啊', '风雨来临时呀啦啦啦起舞', '风雨来临时啊啊啊啊', '风雨来临时呀啦啦啦闪躲', '风雨来临时啊啊啊啊雨点']}\n",
      "{'input_ids': tensor([[     6,  19015, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250030, 250030,\n",
      "         250030, 250029, 250029, 250027, 250029, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  19015, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250030, 250030,\n",
      "         250030, 250029, 250029, 250027, 250029, 250028,      2, 250025]])\n",
      "{'结果 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <m> <m> <m> <m> <d> <d> <h> <d> <l>': ['为何还执着着了的爱着你', '既然没能明了的了开的口', '为何还执着着了的结果里', '终于明白什么了却了果', '终于明白什么了却了点']}\n",
      "{'input_ids': tensor([[     6,  64566, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250027, 250027, 250030, 250030,\n",
      "         250029, 250030, 250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  64566, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250027, 250027, 250030, 250030,\n",
      "         250029, 250030, 250027, 250030,      2, 250025]])\n",
      "{'心情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <m> <h> <h> <m> <m> <d> <m> <h> <m>': ['别让心情随着节拍来', '别让心情随着节拍停', '别让心情随着时间流', '别让心情随着节拍摇', '如今心情随着谁安排']}\n",
      "{'input_ids': tensor([[     6,   9770, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250029, 250030, 250027, 250027,\n",
      "         250027, 250027, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   9770, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250029, 250030, 250027, 250027,\n",
      "         250027, 250027, 250027, 250027,      2, 250025]])\n",
      "{'朋友 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <m> <d> <m> <h> <h> <h> <h> <h> <h>': ['来吧朋友们大家聚一聚', '来吧朋友们相聚相聚', '来吧朋友们相聚相依偎', '来吧朋友们相聚相爱喔', '来吧朋友们相聚相爱哟']}\n",
      "{'input_ids': tensor([[     6,   9524,  14996, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250027, 250030, 250028,\n",
      "         250030, 250030, 250028, 250030, 250028, 250028, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   9524,  14996, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250027, 250030, 250028,\n",
      "         250030, 250030, 250028, 250030, 250028, 250028, 250029,      2, 250025]])\n",
      "{'思念 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <m> <l> <m> <m> <l> <m> <l> <l> <d>': ['思想起从前你离我远了', '关于你从前我总有我的', '思想起从前你离我远吗', '关于你从前我总有你的', '思想起从前你离我远啊']}\n",
      "{'input_ids': tensor([[     6,  91453, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250029, 250030,\n",
      "         250030, 250027, 250030, 250027, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  91453, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250029, 250030,\n",
      "         250030, 250027, 250030, 250027, 250027, 250027,      2, 250025]])\n",
      "{'日子 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <m> <d> <m> <m> <h> <m> <h> <h> <h>': ['未来的男人要活出日子', '那时的平凡幸福那段日子', '未来的男人要活在日子', '未来的男人在同渡日子', '未来的十年未来这日子']}\n",
      "{'input_ids': tensor([[     6,   3221, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250028, 250030,\n",
      "         250030, 250028, 250030, 250028, 250028, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   3221, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250030, 250028, 250030,\n",
      "         250030, 250028, 250030, 250028, 250028, 250029,      2, 250025]])\n",
      "{'世界 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <m> <l> <m> <m> <l> <m> <l> <l> <d>': ['世界很神奇你还想我吗', '世界很神奇你还好我吗', '世界很神奇我能给你的', '为何你从来感觉我好吗', '世界很神奇你还好我呢']}\n",
      "{'input_ids': tensor([[     6, 141390, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250030, 250027, 250028, 250030, 250030,\n",
      "         250029, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 141390, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250030, 250027, 250028, 250030, 250030,\n",
      "         250029, 250028,      2, 250025]])\n",
      "{'时光 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <m> <h> <l> <m> <m> <d> <l>': ['可时光已模糊了你', '把时光锁成的锁', '女时光已模糊了你', '把时光也留成了我', '可时光已模糊了我']}\n",
      "{'input_ids': tensor([[     6,  25099, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250029, 250030, 250027, 250027, 250027,\n",
      "         250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  25099, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250029, 250030, 250027, 250027, 250027,\n",
      "         250027, 250030,      2, 250025]])\n",
      "{'女人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <d> <m> <h> <h> <h> <h> <m>': ['女人一定要坚强', '女人不该太过坚强', '女人从来不怕孤独', '女人不该太过贪婪', '女人不该太过痴迷']}\n",
      "{'input_ids': tensor([[     6, 129954, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250027, 250027, 250030, 250027, 250030, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 129954, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250027, 250027, 250030, 250027, 250030, 250029,      2, 250025]])\n",
      "{'笑容 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <h> <h> <m> <h> <m> <d>': ['用笑容证明了', '那笑容是什么', '用笑容说明了', '让笑容尽情的', '让笑容尽情吧']}\n",
      "{'input_ids': tensor([[     6,  71979, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250030, 250030, 250027, 250027, 250027,\n",
      "         250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  71979, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250030, 250030, 250027, 250027, 250027,\n",
      "         250027, 250030,      2, 250025]])\n",
      "{'爱情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <m> <m> <h> <h> <h> <h> <m>': ['有谁能看透这爱情', '有谁能知道这爱情', '我明白这就是爱情', '有谁能够看透爱情', '有谁能够相信爱情']}\n",
      "{'input_ids': tensor([[     6, 102521, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250028, 250029, 250028, 250030, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 102521, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250028, 250029, 250028, 250030, 250027, 250027,      2, 250025]])\n",
      "{'声音 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <l> <d> <l> <m> <h> <h>': ['你的脸和声音', '你呀你没声音', '你啊你没声音', '你呀你啥声音', '你啊你啥声音']}\n",
      "{'input_ids': tensor([[     6,  26379, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250030, 250027, 250027, 250029,\n",
      "         250029, 250028, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  26379, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250030, 250027, 250027, 250029,\n",
      "         250029, 250028, 250027, 250027,      2, 250025]])\n",
      "{'感觉 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <l> <m> <h> <h> <d> <d> <l> <h> <h>': ['感觉是那么的美妙妙', '感觉是那么的可恶心', '感觉是那么的暖洋洋', '感觉是那么的想放弃', '感觉是那么的很快乐']}\n",
      "{'input_ids': tensor([[     6,   4695, 134479, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250028, 250030, 250027,\n",
      "         250027, 250029, 250027, 250027, 250027, 250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   4695, 134479, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250044, 250032, 250028, 250030, 250027,\n",
      "         250027, 250029, 250027, 250027, 250027, 250027, 250030,      2, 250025]])\n",
      "{'流泪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <l> <m> <h> <h> <d> <h> <h> <h> <h> <m>': ['女流泪为了相恋不分离', '女流泪为了那一份报答', '女流泪为了相恋到天明', '等流泪干了就让它飘零', '等流泪干了就让它飞扬']}\n",
      "{'input_ids': tensor([[     6,  60871, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250030, 250027, 250027, 250029,\n",
      "         250029, 250028, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  60871, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250030, 250027, 250027, 250029,\n",
      "         250029, 250028, 250027, 250027,      2, 250025]])\n",
      "{'时候 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <l> <m> <h> <h> <d> <d> <l> <h> <h>': ['醒来是那么的傻乎乎', '有时候真的了想放弃', '有时候真的了不起劲', '有时候真的了不起意', '有时候真的呀想放弃']}\n",
      "{'input_ids': tensor([[     6,  38717, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250028, 250030, 250027, 250027, 250028, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  38717, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250032, 250028, 250030, 250027, 250027, 250028, 250027,      2, 250025]])\n",
      "{'眼睛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <l> <m> <h> <h> <l> <h>': ['只能闭上眼睛', '仿佛闭上眼睛', '我不要闭眼睛', '你不要闭眼睛', '我迷失在眼睛']}\n",
      "{'input_ids': tensor([[     6,  65503, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250028, 250029, 250028, 250030,\n",
      "         250030, 250030, 250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  65503, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250028, 250029, 250028, 250030,\n",
      "         250030, 250030, 250027, 250030,      2, 250025]])\n",
      "{'梦想 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <m> <l> <d> <l> <m> <m> <m> <h> <m>': ['寻找着属于和和梦想', '寻找着属于童年梦想', '寻找着属于执着追逐', '和我的祖国和谐相连', '寻找着属于别人追逐']}\n",
      "{'input_ids': tensor([[     6,   6717,  11158, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250032, 250030, 250028, 250029, 250028,\n",
      "         250030, 250030, 250030, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   6717,  11158, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250043, 250032, 250030, 250028, 250029, 250028,\n",
      "         250030, 250030, 250030, 250027, 250027,      2, 250025]])\n",
      "{'世间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <m> <l> <d> <l> <m> <m> <m> <h> <h>': ['勇敢的走完茫茫世间', '勇敢的我翱翔于世间', '才懂得远离繁华世间', '勇敢的我从容于世间', '勇敢的我从来没世间']}\n",
      "{'input_ids': tensor([[     6,  10491, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250030, 250027, 250027, 250030, 250030, 250030,\n",
      "         250027, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  10491, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250030, 250027, 250027, 250030, 250030, 250030,\n",
      "         250027, 250029,      2, 250025]])\n",
      "{'地方 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <m> <h> <h> <m> <m> <m> <h> <d>': ['没地方停停停歇歇了', '没地方停停不下的', '同一个儿同一个啊', '没地方停停停歇歇呀', '离地方还得回去吧']}\n",
      "{'input_ids': tensor([[     6, 211284, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250028, 250030, 250028, 250030, 250027, 250030, 250027,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 211284, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250028, 250030, 250028, 250030, 250027, 250030, 250027,\n",
      "              2, 250025]])\n",
      "{'流浪 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <l> <m> <l> <m> <h> <m> <h>': ['我只好流浪流浪', '我可以流浪流浪', '我和你流浪流浪', '我只有一个流浪', '我早已习惯流浪']}\n",
      "{'input_ids': tensor([[     6,  95490, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250027, 250027, 250027, 250030,\n",
      "         250030, 250030, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  95490, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250027, 250027, 250027, 250030,\n",
      "         250030, 250030, 250027, 250027,      2, 250025]])\n",
      "{'天空 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <l> <h> <h> <h> <m> <m> <m> <h> <h>': ['仰望天空翱翔蓝天下', '仰望天空翱翔和歌唱', '仰望天空翱翔和放纵', '仰望天空翱翔蓝天上', '仰望天空遨游蓝天下']}\n",
      "{'input_ids': tensor([[     6,   1801,  12392, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250028, 250027, 250027, 250030, 250030, 250030,      2,\n",
      "         250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   1801,  12392, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250032, 250028, 250027, 250027, 250030, 250030, 250030,      2,\n",
      "         250025]])\n",
      "{'心痛 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <|num|> <l> <h> <h> <m> <m> <m>': ['有心痛才明白', '把心痛来埋藏', '把心痛来折磨', '把心痛来形容', '把心痛埋藏藏']}\n",
      "{'input_ids': tensor([[     6,   5477, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250027, 250027, 250027, 250030,\n",
      "         250030, 250030, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   5477, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250027, 250027, 250027, 250030,\n",
      "         250030, 250030, 250027, 250027,      2, 250025]])\n",
      "{'时间 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <l> <h> <h> <h> <m> <m> <m> <h> <h>': ['只是因为没和时间说', '只是因为没钱和时间', '我知道爱没随时间去', '我知道该和和时间赛', '我知道该和随时间去']}\n",
      "{'input_ids': tensor([[     6,  13599, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250027, 250027, 250030, 250028, 250027, 250029, 250028,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  13599, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250027, 250027, 250030, 250028, 250027, 250029, 250028,\n",
      "              2, 250025]])\n",
      "{'人生 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <h> <h> <m> <l> <h> <d> <l>': ['面对人生的苦', '漫漫人生的路啊走', '笑看人生的苦', '漫漫人生的路啊你', '漫漫人生的路啊我']}\n",
      "{'input_ids': tensor([[     6,  17959,   1801, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029,\n",
      "         250029, 250030, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  17959,   1801, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029,\n",
      "         250029, 250030, 250028,      2, 250025]])\n",
      "{'伤心 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <h> <d> <m> <d> <d> <m> <l>': ['掩饰着人们的愚蠢', '我伤了难得的所有', '掩饰着人们的惶恐', '我伤了难了的情感', '我伤了难了的完美']}\n",
      "{'input_ids': tensor([[     6,   7558,    487, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029,\n",
      "         250029, 250027, 250027,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,   7558,    487, 250034, 250035, 250036, 250037, 250038, 250039,\n",
      "         250040, 250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029,\n",
      "         250029, 250027, 250027,      2, 250025]])\n",
      "{'爱人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <h> <d> <m> <d> <d> <h> <h>': ['等待着孩子的爱人', '守护着孩子的爱人', '想念着咱们的爱人', '守候着孩子的爱人', '守望着孩子的爱人']}\n",
      "{'input_ids': tensor([[     6,  29476, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029, 250029,\n",
      "         250030, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  29476, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250032, 250028, 250027, 250029, 250030, 250029, 250029,\n",
      "         250030, 250028,      2, 250025]])\n",
      "{'感情 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <|num|> <l> <h> <d> <m> <d> <d> <m> <l>': ['毁灭了人们的感情', '掩盖了人们的感情', '考验着人们的感情', '守候着人们的感情', '掩饰了人们的感情']}\n",
      "{'input_ids': tensor([[     6,  84460, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250030, 250030, 250028, 250029,\n",
      "         250030, 250030, 250028, 250029, 250030, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  84460, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250030, 250030, 250028, 250029,\n",
      "         250030, 250030, 250028, 250029, 250030, 250030,      2, 250025]])\n",
      "{'记忆 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <m> <m> <l> <d> <m> <m> <l> <d> <m> <m>': ['成为你的童年你的童年', '模糊我的童年我的童年', '童年里的童年你的童年', '回想起了童年里的童年', '回想起了从前你的童年']}\n",
      "{'input_ids': tensor([[     6,  41029, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250027, 250030, 250029, 250030, 250027,\n",
      "         250028, 250030, 250028, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  41029, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250027, 250030, 250029, 250030, 250027,\n",
      "         250028, 250030, 250028, 250029,      2, 250025]])\n",
      "{'习惯 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <h> <m> <d> <m> <h> <l> <m> <l> <d>': ['幸福的习惯我改写了', '幸福的习惯你改改了', '幸福的习惯你改改吧', '生活的习惯我改写了', '幸福的习惯我改改了']}\n",
      "{'input_ids': tensor([[     6, 162253, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250030, 250029, 250027, 250030,\n",
      "         250030, 250030, 250030, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 162253, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250030, 250030, 250029, 250027, 250030,\n",
      "         250030, 250030, 250030, 250030,      2, 250025]])\n",
      "{'滋味 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <m> <m> <d> <h> <m> <m> <m> <m> <m>': ['离别的滋味谁能明白', '离别的滋味如何形容', '缠绵的滋味谁能明白', '离别的滋味无人能尝', '离别的滋味没人能尝']}\n",
      "{'input_ids': tensor([[     6, 142270, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250027, 250030, 250030,\n",
      "         250030, 250030, 250030, 250030, 250030, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 142270, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250027, 250027, 250030, 250030,\n",
      "         250030, 250030, 250030, 250030, 250030, 250029,      2, 250025]])\n",
      "{'情人 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <h> <h> <m> <m> <m> <m> <m> <m> <m> <d>': ['那些情人何时才能明了', '不知何时情人才回来了', '做个情人其实还没什么', '做个情人其实没什么', '不知何时才能情人陪着']}\n",
      "{'input_ids': tensor([[     6, 178203, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250045, 250032, 250028, 250030, 250030,\n",
      "         250027, 250027, 250027, 250027, 250027, 250027, 250030, 250030,      2,\n",
      "         250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 178203, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250045, 250032, 250028, 250030, 250030,\n",
      "         250027, 250027, 250027, 250027, 250027, 250027, 250030, 250030,      2,\n",
      "         250025]])\n",
      "{'勇气 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <c10> <|num|> <l> <m> <m> <h> <h> <h> <h> <h> <h> <m> <m>': ['把执着当作是那份勇气无敌', '把执着当作是那勇气是无敌', '把执着当作是那份勇气无穷', '把执着当作是那勇气是执着', '把执着当作是那勇气在传承']}\n",
      "{'input_ids': tensor([[     6,  17246, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250028, 250029, 250028, 250027,\n",
      "         250030, 250030, 250030, 250030, 250030, 250029,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  17246, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250028, 250029, 250028, 250027,\n",
      "         250030, 250030, 250030, 250030, 250030, 250029,      2, 250025]])\n",
      "{'故事 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <l> <d> <l> <h> <m> <m> <m> <m> <m> <d>': ['你的故事没人能明了', '你的故事从来没人明了', '你的故事从来没别人的', '我的故事没人能明了', '你的故事从来没完结了']}\n",
      "{'input_ids': tensor([[     6,  63112, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250028, 250029, 250028, 250027,\n",
      "         250030, 250028, 250030, 250028, 250027, 250028,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6,  63112, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250044, 250032, 250028, 250029, 250028, 250027,\n",
      "         250030, 250028, 250030, 250028, 250027, 250028,      2, 250025]])\n",
      "{'青春 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <c9> <|num|> <l> <d> <l> <h> <m> <l> <m> <l> <h> <l>': ['我们把青春留给彼此', '有了你生活也才有青春', '有了你生活也无悔青春', '我的眼泪流淌成我青春', '有了你生活也得有青春']}\n",
      "{'input_ids': tensor([[     6, 112895, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250029, 250028, 250027, 250030,\n",
      "         250027, 250027, 250027, 250030,      2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 112895, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250042, 250043, 250032, 250028, 250029, 250028, 250027, 250030,\n",
      "         250027, 250027, 250027, 250030,      2, 250025]])\n",
      "{'美丽 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <c7> <c8> <|num|> <l> <d> <l> <h> <m> <h> <h> <h> <m>': ['你的美丽迷醉在心头', '你的美丽迷恋在心头', '你的美丽还在不在前', '你的美丽和那份温柔', '你的美丽和那份善良']}\n",
      "{'input_ids': tensor([[     6, 119715, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250027, 250030, 250027, 250027, 250027, 250029, 250030,\n",
      "              2, 250025]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[     2, 250025]])}\n",
      "tensor([[     6, 119715, 250034, 250035, 250036, 250037, 250038, 250039, 250040,\n",
      "         250041, 250032, 250027, 250030, 250027, 250027, 250027, 250029, 250030,\n",
      "              2, 250025]])\n",
      "{'命运 <|kwd|> <c0> <c1> <c2> <c3> <c4> <c5> <c6> <|num|> <h> <m> <h> <h> <h> <d> <m>': ['是谁在命运的牢', '为何命运那么难', '是谁是命运的船', '为何命运这么难', '是谁在命运的河']}\n"
     ]
    }
   ],
   "source": [
    "three_height_generation = list()\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    word_list = [prompts[i]]\n",
    "    emb_list = []\n",
    "    for item in word_list:\n",
    "        emb_list.append({'translation': {'hi': item,'en': ''}})\n",
    "    A = data_collator(emb_list)\n",
    "    print(A)\n",
    "    print(A['input_ids'])\n",
    "\n",
    "    fea_output = {}\n",
    "    for i in range(A['input_ids'].shape[0]):\n",
    "        B = torch.unsqueeze(A['input_ids'][i],0)\n",
    "        output = mbart_model.generate(B.to(device='cuda'), max_length=500,num_beams=5, num_return_sequences=5)\n",
    "        fea_output[word_list[i]] = tokenizer.batch_decode(output, skip_special_tokens=True,temperature=0.5)\n",
    "    #     print(word_list[i],tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "        print(fea_output)\n",
    "    three_height_generation.append(fea_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3e549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑夜 有了你黑夜远离 1\n",
      "无情 你的每句无情句 1\n",
      "泪水 你的眼中泪水流着 1\n",
      "眼神 眼神迷了眼 1\n",
      "风雨 风雨来临时啊啊啊啊 2\n",
      "结果 为何还执着着了的结果里 2\n",
      "心情 别让心情随着时间流 2\n",
      "朋友 来吧朋友们相聚相依偎 2\n",
      "思念 思想起从前你离我远吗 3\n",
      "日子 未来的男人要活在日子 3\n",
      "世界 世界很神奇我能给你的 3\n",
      "时光 女时光已模糊了你 3\n",
      "女人 女人从来不怕孤独 4\n",
      "笑容 用笑容说明了 4\n",
      "爱情 我明白这就是爱情 4\n",
      "声音 你啊你没声音 4\n",
      "感觉 感觉是那么的暖洋洋 5\n",
      "流泪 女流泪为了相恋到天明 5\n",
      "时候 有时候真的了不起劲 5\n",
      "眼睛 我不要闭眼睛 5\n",
      "梦想 寻找着属于执着追逐 6\n",
      "世间 才懂得远离繁华世间 6\n",
      "地方 同一个儿同一个啊 6\n",
      "流浪 我和你流浪流浪 6\n",
      "天空 仰望天空翱翔和放纵 7\n",
      "心痛 把心痛来折磨 7\n",
      "时间 我知道爱没随时间去 7\n",
      "人生 笑看人生的苦 7\n",
      "伤心 掩饰着人们的惶恐 8\n",
      "爱人 想念着咱们的爱人 8\n",
      "感情 考验着人们的感情 8\n",
      "记忆 童年里的童年你的童年 8\n",
      "习惯 幸福的习惯你改改吧 9\n",
      "滋味 缠绵的滋味谁能明白 9\n",
      "情人 做个情人其实还没什么 9\n",
      "勇气 把执着当作是那份勇气无穷 9\n",
      "故事 你的故事从来没别人的 10\n",
      "青春 有了你生活也无悔青春 10\n",
      "美丽 你的美丽还在不在前 10\n",
      "命运 是谁是命运的船 10\n"
     ]
    }
   ],
   "source": [
    "for x in three_height_generation:\n",
    "    keyword = list(x.keys())[0].split(\"<|kwd|>\")[0][:-1]\n",
    "    sent = list(x.values())[0][2]\n",
    "    for k in keywords2midi.keys():\n",
    "        if keyword in k:\n",
    "            idx = keywords2midi[k]\n",
    "    print(keyword,sent,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86cf46e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fec68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"4_height_num_wb_epoch\",overwrite_output_dir=True,\n",
    "                        do_train=True,\n",
    "                        do_eval=True,\n",
    "                        save_steps=100000,\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=32,\n",
    "                        per_device_eval_batch_size=32,\n",
    "                        learning_rate=5e-5,\n",
    "                        num_train_epochs=3,\n",
    "                        logging_dir=\"logs\")\n",
    "\n",
    "# defining trainer using 🤗\n",
    "trainer = Seq2SeqTrainer(model=mbart_model, \n",
    "                args=args, \n",
    "                data_collator=data_collator, \n",
    "                train_dataset=train_dataset, \n",
    "                eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4399551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31b5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54091ee",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca70d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'黑夜,无情,泪水,眼神': 1, '风雨,结果,心情,朋友': 2, '思念,日子,世界,时光': 3, '女人,笑容,爱情,声音': 4, '感觉,流泪,时候,眼睛': 5, '梦想,世间,地方,流浪': 6, '天空,心痛,时间,人生': 7, '伤心,爱人,感情,记忆': 8, '习惯,滋味,情人,勇气': 9, '故事,青春,美丽,命运': 10}\n"
     ]
    }
   ],
   "source": [
    "midi2keywords = {1: '黑夜,无情,泪水,眼神',\n",
    " 2: '风雨,结果,心情,朋友',\n",
    " 3: '思念,日子,世界,时光',\n",
    " 4: '女人,笑容,爱情,声音',\n",
    " 5: '感觉,流泪,时候,眼睛',\n",
    " 6: '梦想,世间,地方,流浪',\n",
    " 7: '天空,心痛,时间,人生',\n",
    " 8: '伤心,爱人,感情,记忆',\n",
    " 9: '习惯,滋味,情人,勇气',\n",
    " 10: '故事,青春,美丽,命运'}\n",
    "\n",
    "def swap_keys_values(dictionary):\n",
    "    swapped_dict = {value: key for key, value in dictionary.items()}\n",
    "    return swapped_dict\n",
    "\n",
    "keywords2midi = swap_keys_values(midi2keywords)\n",
    "print(keywords2midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7aedd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_lyrics(prompt,model):\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "    sample_outputs = model.generate(\n",
    "                            generated, \n",
    "                            #bos_token_id=random.randint(1,30000),\n",
    "                            do_sample=True,   \n",
    "                            top_k=10, \n",
    "                            max_length = 300,\n",
    "                            top_p=0.95, \n",
    "                            num_return_sequences=1\n",
    "                            )\n",
    "\n",
    "    res = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
